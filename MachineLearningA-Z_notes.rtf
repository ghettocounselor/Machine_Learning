{\rtf1\ansi\ansicpg1252\cocoartf1561\cocoasubrtf600
{\fonttbl\f0\fswiss\fcharset0 Helvetica;\f1\froman\fcharset0 Times-Roman;}
{\colortbl;\red255\green255\blue255;\red0\green0\blue233;\red31\green36\blue45;\red255\green255\blue255;
}
{\*\expandedcolortbl;;\cssrgb\c0\c0\c93333;\cssrgb\c16078\c18824\c23137;\cssrgb\c100000\c100000\c100000;
}
{\*\listtable{\list\listtemplateid1\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid1\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid1}}
{\*\listoverridetable{\listoverride\listid1\listoverridecount0\ls1}}
\margl1440\margr1440\vieww23300\viewh13900\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 For Python we\'92ll use Sypder IDE, launched from Anaconda\
\
Datasets\
\pard\pardeftab720\sl280\partightenfactor0
{\field{\*\fldinst{HYPERLINK "https://www.superdatascience.com/pages/machine-learning"}}{\fldrslt 
\f1 \cf2 \expnd0\expndtw0\kerning0
\ul \ulc2 https://www.superdatascience.com/pages/machine-learning}}
\f1 \cf2 \expnd0\expndtw0\kerning0
\ul \ulc2 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0 \cf0 \kerning1\expnd0\expndtw0 \ulnone \
In Machine Learning there\'92s always a dependent variable.\
We\'92ll include other independent variables to determine which independent variables impact the dependent variable. \
\
Lectures 10-13 basics of getting data into Python/Spyder and R\
https://www.udemy.com/machinelearning/learn/lecture/5859238\
\
Couple of files to note here;\
1. data_preprocessing_template.py - out of the box file, need a few of the bits from other OOB files to make this really work.\
2. data_preprocessing_template.R - out of the box file, need a few of the bits from other OOB files to make this really work.\
3. data_preprocessing_template_NOTES.py - full complement of notes and code, fully functional\
4. data_preprocessing_template_NOTES.R - full complement of notes and code, fully functional\
\
Lecture 15 Missing data in dataset\
General rule of replacing missing data: take the mean of all other variables\
walked through replacing blank fields in Python/Spyder and R with the Mean of other variables.\
\
Lecture 16 Categorical Data\
Here we\'92ll encode categorical data\
In our data we have two categorical variables\
Since Models are built on numbers categorical variables cannot be used\
we need to encode them into numbers. \
\
Lecture 18 Splitting Dataset into training and test :D\
https://www.udemy.com/machinelearning/learn/lecture/5683430 \
Why separate?\
We need a test set and a train set. \
We build on train and test on test. \
This is so the Model is understanding the correlations of the data and prove it\'92s knowledge on the Test data. \
\
Part two - Regression - Lectures 21 - 81 !!\
\pard\pardeftab720\partightenfactor0
\cf3 \cb4 \expnd0\expndtw0\kerning0
Welcome to Part 2 - Regression\cb1 \
\
\cb4 Regression models (both linear and non-linear) are used for predicting a real value, like salary for example. If your\'a0independent variable is time, then you are forecasting future values, otherwise your model is predicting present but\'a0unknown values. Regression technique vary from Linear Regression to SVR and Random\'a0Forests Regression.\cb1 \
\cb4 In this part, you will understand and learn how to implement the following Machine Learning Regression models:\cb1 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls1\ilvl0\cf3 \cb4 \kerning1\expnd0\expndtw0 {\listtext	1	}Section 4 - \expnd0\expndtw0\kerning0
Simple Linear Regression - Lecture 22 - 33\cb1 \
\ls1\ilvl0\cb4 \kerning1\expnd0\expndtw0 {\listtext	2	}\cf3 \cb4 Section 5 - \cf3 \cb4 \expnd0\expndtw0\kerning0
Multiple Linear Regression\cf3 \cb4  - Lecture 34 - 54\cf3 \cb1 \
\ls1\ilvl0\cb4 \kerning1\expnd0\expndtw0 {\listtext	3	}\cf3 \cb4 Section 6 - \cf3 \cb4 \expnd0\expndtw0\kerning0
Polynomial Regression\cf3 \cb4  - Lecture 55 - 66\cf3 \cb1 \
\ls1\ilvl0\cb4 \kerning1\expnd0\expndtw0 {\listtext	4	}\cf3 \cb4 Section 7 - \cf3 \cb4 \expnd0\expndtw0\kerning0
Support Vector for Regression (SVR)\cf3 \cb4  - Lecture 67 - 70\cf3 \cb1 \
\ls1\ilvl0\cb4 \kerning1\expnd0\expndtw0 {\listtext	5	}\cf3 \cb4 Section 8 - \cf3 \cb4 \expnd0\expndtw0\kerning0
Decision Tree Classification\cf3 \cb4  - Lecture 71 - 74\cf3 \cb1 \
\ls1\ilvl0\cb4 \kerning1\expnd0\expndtw0 {\listtext	6	}\cf3 \cb4 Section 9 - \cf3 \cb4 \expnd0\expndtw0\kerning0
Random Forest Classification\cf3 \cb4  - Lecture 75 - 78\
}