---
title: "Kernel SVM - machine learning in R"
output: html_document
---
Github files; https://github.com/ghettocounselor

### Kernel SVM - Support Vector Machine (SVM)
lecture 108 Kernel SVM intuition (what's this about) - basic idea is
not everything is linear :) 
https://www.udemy.com/machinelearning/learn/lecture/6113144

lecture 109 Mapping higher dimensions
https://www.udemy.com/machinelearning/learn/lecture/6113148

lecture 110 The Kernel Trick - The Gaussian RBF Kernel example
https://www.udemy.com/machinelearning/learn/lecture/6113150

lecture 111 - Types of functions (Kernels) - see image
https://www.udemy.com/machinelearning/learn/lecture/6113152

Lots of examples of Kernel application see Google Images search 
search https://www.google.com/search?biw=1409&bih=758&tbm=isch&sa=1&ei=J2PhXMHJJtDUsAW_n6_AAQ&q=machine+learning+kernel+example&oq=machine+learning+kernel+example&gs_l=img.12...0.0..15042...0.0..0.0.0.......1......gws-wiz-img.v0zg2PyWs6o

Basic idea here is that not everything (data) can be separated by a linear model. Because of this we need more complex processing; one step is to "Map Higher Dimensions" This process will 'increase the dimensionality'. In order to not consume huge computational resources we can use The Kernel Trick! Which really simply means we'll use a Kernel to do the work, no trick at all!

Check Working directory getwd() to always know where you are working. 
```{r, include=FALSE}
getwd()
```

# Importing the dataset
we are after the age and salary and the y/n purchased
so in R that's columns 3-5
```{r , include=TRUE}
dataset = read.csv('Social_Network_Ads.csv')
dataset = dataset[3:5]
```
Have a look at data
```{r , include=TRUE}
summary(dataset)
head(dataset)
```

# Encoding the target feature as factor
```{r , include=TRUE}
dataset$Purchased = factor(dataset$Purchased, levels = c(0, 1))
```
Let's look again
```{r , include=TRUE}
summary(dataset)
```

# Splitting the dataset into the Training set and Test set
General rule of thumb is 75% for split ratio; 75% train, 25% test
```{r , include=TRUE}
# install.packages('caTools')
library(caTools)
set.seed(123)
split = sample.split(dataset$Purchased, SplitRatio = 0.75)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
```

# Feature Scaling
Feature Scaling - for classification it's better to do feature scalling
additionally we have variables where the units are not the same
```{r , include=TRUE}
training_set[-3] = scale(training_set[-3])
test_set[-3] = scale(test_set[-3])
```
Let's have a look.
```{r , include=TRUE}
head(training_set)
```

# Fitting SVM to the Training set
We'll use kernel = Radial Basis (RBF) as our example this time, but keep Linear as comparison.

kernel - the kernel used in training and predicting. You might consider changing some of the following parameters, depending on the kernel type.
Options; linear, polynomial, radial basis (radial or RBF in Python), sigmoid.

The formula = the dependent variable ~ other variables you want to test in this case '.' for all. 
data = data you want to train on
```{r , include=TRUE}
# install.packages('e1071')
library(e1071)
classifierR = svm(formula = Purchased ~ .,
                 data = training_set,
                 type = 'C-classification', # this is because we want to make a regression classification
                 kernel = 'radial')
```

Leave linear in code for comparison.
```{r , include=TRUE}
# install.packages('e1071')
library(e1071)
classifierL = svm(formula = Purchased ~ .,
                 data = training_set,
                 type = 'C-classification',
                 kernel = 'linear')
```

# Predict the Test set results - Radial (Gaussian)
Very cool stuf out there to visually understand what is going on here. 

https://en.wikipedia.org/wiki/Euclidean_space   

https://en.wikipedia.org/wiki/Gaussian_elimination

```{r , include=TRUE}
y_predR = predict(classifierR, newdata = test_set[-3])
```

# Predict the Test set results - Linear
```{r , include=TRUE}
y_predL = predict(classifierL, newdata = test_set[-3])
```

# Making the Confusion Matrix - Radial
```{r , include=TRUE}
cmR = table(test_set[, 3], y_predR)
cmR
```

# Making the Confusion Matrix - Linear
```{r , include=TRUE}
cmL = table(test_set[, 3], y_predL)
cmL
```

# Visualising the Training set results - Radial
```{r , include=TRUE, fig.width=10, fig.height=7}
library(ElemStatLearn)
# declare set as the training set
set = training_set
# this section creates the background region red/green. It does that by the 'by' which you can think of as the steps in python, so each 0.01 is interpreted as 0 or 1 and is either green or red. The -1 and +1 give us the space around the edges so the dots are not jammed
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
# just giving a name to the X and Y 
colnames(grid_set) = c('Age', 'EstimatedSalary')
# this is the MAGIC of the background coloring
# here we use the classifier to predict the result of each of each of the pixel bits noted above
y_gridR = predict(classifierR, newdata = grid_set)
# that's the end of the background
# now we plat the actual data 
plot(set[, -3],
     main = 'SVM Radial Kernel (Training set)',
     xlab = 'Age', ylab = 'Estimated Salary',
     xlim = range(X1), ylim = range(X2)) # this bit creates the limits to the values plotted this is also a part of the MAGIC as it creates the line between green and red
contour(X1, X2, matrix(as.numeric(y_gridR), length(X1), length(X2)), add = TRUE)
# here we run through all the y_pred data and use ifelse to color the dots
# note the dots are the real data, the background is the pixel by pixel determination of y/n
# graph the dots on top of the background give you the image
points(grid_set, pch = '.', col = ifelse(y_gridR == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
```

# Visualising the Test set results - Radial
```{r , include=TRUE, fig.width=10, fig.height=7}
library(ElemStatLearn)
set = test_set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
y_gridR = predict(classifierR, newdata = grid_set)
plot(set[, -3], main = 'SVM Radial Kernel (Test set)',
     xlab = 'Age', ylab = 'Estimated Salary',
     xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_gridR), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_gridR == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
```

# Visualising the Training set results - Linear
```{r , include=TRUE, fig.width=10, fig.height=7}
library(ElemStatLearn)
# declare set as the training set
set = training_set
# this section creates the background region red/green. It does that by the 'by' which you can think of as the steps in python, so each 0.01 is interpreted as 0 or 1 and is either green or red. The -1 and +1 give us the space around the edges so the dots are not jammed
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
# just giving a name to the X and Y 
colnames(grid_set) = c('Age', 'EstimatedSalary')
# this is the MAGIC of the background coloring
# here we use the classifier to predict the result of each of each of the pixel bits noted above
y_gridL = predict(classifierL, newdata = grid_set)
# that's the end of the background
# now we plat the actual data 
plot(set[, -3],
     main = 'SVM Linear Kernel (Training set)',
     xlab = 'Age', ylab = 'Estimated Salary',
     xlim = range(X1), ylim = range(X2)) # this bit creates the limits to the values plotted this is also a part of the MAGIC as it creates the line between green and red
contour(X1, X2, matrix(as.numeric(y_gridL), length(X1), length(X2)), add = TRUE)
# here we run through all the y_pred data and use ifelse to color the dots
# note the dots are the real data, the background is the pixel by pixel determination of y/n
# graph the dots on top of the background give you the image
points(grid_set, pch = '.', col = ifelse(y_gridL == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
```

# Visualising the Test set results - Linear
```{r , include=TRUE, fig.width=10, fig.height=7}
library(ElemStatLearn)
set = test_set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
y_gridL = predict(classifierL, newdata = grid_set)
plot(set[, -3], main = 'SVM Linear Kernel (Test set)',
     xlab = 'Age', ylab = 'Estimated Salary',
     xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_gridL), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_gridL == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
```