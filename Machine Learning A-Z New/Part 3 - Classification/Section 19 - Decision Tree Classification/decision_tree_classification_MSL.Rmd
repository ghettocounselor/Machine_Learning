---
title: "Decision Tree algorithm in a prediction model"
output: html_document
---

Github files; https://github.com/ghettocounselor

### Decision Tree
Intuition Lecture 122 https://www.udemy.com/machinelearning/learn/lecture/5714410

Lecutre 125 https://www.udemy.com/machinelearning/learn/lecture/5759486 

Decision tree algorithms are about splitting the data into classifications to then have an algorithm that will predict where new points of data will land. Those classifications are based on values of the independent and dependent variables. 

Check Working directory getwd() to always know where you are working. 
```{r, include=FALSE}
getwd()
```

# Importing the dataset
we are after the age and salary and the y/n purchased
so in R that's columns 3-5
```{r , include=TRUE}
dataset = read.csv('Social_Network_Ads.csv')
dataset = dataset[3:5]
```
Have a look at data
```{r , include=TRUE}
summary(dataset)
head(dataset)
```

# Encoding the target feature, catagorical variable, as factor
We do this remember because the model we are using doesn't do this for us.
```{r , include=TRUE}
dataset$Purchased = factor(dataset$Purchased, levels = c(0, 1))
```
Let's look again
```{r , include=TRUE}
summary(dataset)
```

# Splitting the dataset into the Training set and Test set
General rule of thumb is 75% for split ratio; 75% train, 25% test
```{r , include=TRUE}
# install.packages('caTools')
library(caTools)
set.seed(123)
split = sample.split(dataset$Purchased, SplitRatio = 0.75)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
```

# Feature Scaling
Feature Scaling - for classification it's better to do feature scalling
additionally we have variables where the units are not the same. For decision trees we don't need to do this because the model is not based on euclidian distances, however it will make the graphing faster. 
```{r , include=TRUE}
training_set[-3] = scale(training_set[-3])
test_set[-3] = scale(test_set[-3])
```
Let's have a look.
```{r , include=TRUE}
head(training_set)
```

# Fitting Decision Tree to the Training set
Things are a little different here, we don't need formula, and other features we just need x and y. x will be the independent variables (hence the datase -3 removing the column we don't need), y is the dependent variable. 
```{r , include=TRUE}
# install.packages('rpart')
library(rpart)
classifier = rpart(formula = Purchased ~ .,
                   data = training_set)
```

# Predict the Test set results - Decision Tree 
Because of the slight variation in structure of the decision tree we need to add the type = class. 
```{r , include=TRUE}
y_pred = predict(classifier, newdata = test_set[-3])
```
Note; it's a Matrix!
And it's a matrix of probabilities, column 0 is the probability the user will buy the SUV and column 1 is the probability that the user would buy the SUV. 
```{r , include=TRUE}
head(y_pred)
```

Let's fix that. 
```{r , include=TRUE}
y_pred = predict(classifier, newdata = test_set[-3], type = 'class')
```
Let's look
```{r , include=TRUE}
y_pred
```


# Making the Confusion Matrix - Decision Tree 
Now we have the normal CM because we added the class
```{r , include=TRUE}
cm = table(test_set[, 3], y_pred)
cm
```

# Visualising the Training set results - Decision Tree 
```{r , include=TRUE, fig.width=10, fig.height=7}
library(ElemStatLearn)
# declare set as the training set
set = training_set
# this section creates the background region red/green. It does that by the 'by' which you can think of as the steps in python, so each 0.01 is interpreted as 0 or 1 and is either green or red. The -1 and +1 give us the space around the edges so the dots are not jammed. Another way to think of the 'by' as is as the resolution of the graphing of the background
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
# just giving a name to the X and Y 
colnames(grid_set) = c('Age', 'EstimatedSalary')
# this is the MAGIC of the background coloring
# here we use the classifier to predict the result of each of each of the pixel bits noted above. NOTE we need class here because we have a y_grid is a matrix!
y_grid = predict(classifier, newdata = grid_set, type = 'class')
# that's the end of the background
# now we plat the actual data 
plot(set[, -3],
     main = 'Decision Tree (Training set)',
     xlab = 'Age', ylab = 'Estimated Salary',
     xlim = range(X1), ylim = range(X2)) # this bit creates the limits to the values plotted this is also a part of the MAGIC as it creates the line between green and red
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
# here we run through all the y_pred data and use ifelse to color the dots
# note the dots are the real data, the background is the pixel by pixel determination of y/n
# graph the dots on top of the background give you the image
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
```

# Visualising the Test set results - Decision Tree 
```{r , include=TRUE, fig.width=10, fig.height=7}
library(ElemStatLearn)
set = test_set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
# NOTE we need class here because we have a y_grid is a matrix!
y_grid = predict(classifier, newdata = grid_set, type = 'class')
plot(set[, -3], main = 'Decision Tree (Test set)',
     xlab = 'Age', ylab = 'Estimated Salary',
     xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
```

# Added bonus let's visualize the Decsion Trees
However we need to take the feature scaling out so we can read the splits :D

```{r , include=TRUE}
dataset = read.csv('Social_Network_Ads.csv')
dataset = dataset[3:5]
```

```{r , include=TRUE}
dataset$Purchased = factor(dataset$Purchased, levels = c(0, 1))
```

```{r , include=TRUE}
# install.packages('caTools')
library(caTools)
set.seed(123)
split = sample.split(dataset$Purchased, SplitRatio = 0.75)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
```

Plotting the tree
```{r , include=TRUE, fig.width=10, fig.height=7}
plot(classifier)
text(classifier)
```