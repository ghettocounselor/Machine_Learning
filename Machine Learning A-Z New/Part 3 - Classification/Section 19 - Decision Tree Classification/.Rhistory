AMZN.EMA.10 <- EMA(AMZN$AMZN.Close, n=10 )
AMZN.EMA.50 <- EMA(AMZN$AMZN.Close, n=50 )
AMZN.EMA.200 <- EMA(AMZN$AMZN.Close, n=200 )
AMZN.Fast.Diff <- AMZN.EMA.10 - AMZN.EMA.50
AMZN.Slow.Diff <- AMZN.EMA.50 - AMZN.EMA.200
addTA(AMZN.Fast.Diff, col='blue', type='h',legend="10-50 MA used for in-out of market")
addTA(AMZN.Slow.Diff, col='red', type='h',legend="50-200 MA give trending sense")
addTA=c(addMACD(),addVo(),addADX(n = 14, maType = "EMA"))
chartSeries(AMZN, theme=chartTheme('white'),
type = c("auto", "matchsticks"),
subset = '2016-01::',
show.grid = TRUE,
major.ticks='auto', minor.ticks=TRUE,
multi.col = FALSE,
TA="addEMA(50, col='black');addEMA(200, col='blue')")
plot(AMZN)
chartSeries(AMZN, theme=chartTheme('white'),
type = c("auto", "matchsticks"),
subset = '2016-01::',
show.grid = TRUE,
major.ticks='auto', minor.ticks=TRUE,
multi.col = FALSE,
TA="addEMA(50, col='black');addEMA(200, col='blue')")
plot(AMZN$AMZN.Close)
plot(GSPC$GSPC.Close)
GSPC.Long_Trades <- ifelse(
GSPC.SlowS.Diff > 0 &
GSPC.FastS.Diff > 0 &
shift(v=as.numeric(GSPC.FastS.Diff), places=1,dir = 'right') < 0,
GSPC$GSPC.Close, NA)
GSPC.Short_Trades <- ifelse(
GSPC.SlowS.Diff < 0 &
GSPC.FastS.Diff < 0 &
shift(v=as.numeric(GSPC.FastS.Diff), places=1,dir = 'right') > 0,
GSPC$GSPC.Close, NA)
plot(GSPC$GSPC.Close)
points(GSPC.Long_Trades, col='blue', cex=1.5, pch=18)
points(GSPC.Short_Trades, col='red', cex=1.5, pch=18)
GSPC.Long_Trades <- ifelse(
GSPC.SlowS.Diff > 0 &
GSPC.FastS.Diff > 0 &
shift(v=as.numeric(GSPC.FastS.Diff), places=1,dir = 'right') < 0,
GSPC$GSPC.Close, NA)
GSPC.Short_Trades <- ifelse(
GSPC.SlowS.Diff < 0 &
GSPC.FastS.Diff < 0 &
shift(v=as.numeric(GSPC.FastS.Diff), places=1,dir = 'right') > 0,
GSPC$GSPC.Close, NA)
plot(GSPC$GSPC.Close)
points(GSPC.Long_Trades, col='blue', cex=1.5, pch=18)
points(GSPC.Short_Trades, col='red', cex=1.5, pch=18)
addTA(GSPC.Fast.Diff, col='blue', type='h',legend="10-50 MA used for in-out of market")
addTA(GSPC.Slow.Diff, col='red', type='h',legend="50-200 MA give trending sense")
addTA=c(addMACD(),addVo(),addADX(n = 14, maType = "EMA"))
GSPC.Long_Trades <- ifelse(
GSPC.SlowS.Diff > 0 &
GSPC.FastS.Diff > 0 &
shift(v=as.numeric(GSPC.FastS.Diff), places=1,dir = 'right') < 0,
GSPC$GSPC.Close, NA)
GSPC.Short_Trades <- ifelse(
GSPC.SlowS.Diff < 0 &
GSPC.FastS.Diff < 0 &
shift(v=as.numeric(GSPC.FastS.Diff), places=1,dir = 'right') > 0,
GSPC$GSPC.Close, NA)
plot(GSPC$GSPC.Close)
points(GSPC.Long_Trades, col='blue', cex=1.5, pch=18)
points(GSPC.Short_Trades, col='red', cex=1.5, pch=18)
addTA(GSPC.Fast.Diff, col='blue', type='h',legend="10-50 MA used for in-out of market")
addTA(GSPC.Slow.Diff, col='red', type='h',legend="50-200 MA give trending sense")
addTA=c(addMACD(),addVo(),addADX(n = 14, maType = "EMA"))
GSPC.Long_Trades <- ifelse(
GSPC.SlowS.Diff > 0 &
GSPC.FastS.Diff > 0 &
shift(v=as.numeric(GSPC.FastS.Diff), places=1,dir = 'right') < 0,
GSPC$GSPC.Close, NA)
GSPC.Short_Trades <- ifelse(
GSPC.SlowS.Diff < 0 &
GSPC.FastS.Diff < 0 &
shift(v=as.numeric(GSPC.FastS.Diff), places=1,dir = 'right') > 0,
GSPC$GSPC.Close, NA)
plot(GSPC$GSPC.Close)
points(GSPC.Long_Trades, col='blue', cex=1.5, pch=18)
points(GSPC.Short_Trades, col='red', cex=1.5, pch=18)
addTA(GSPC.Fast.Diff, col='blue', type='h',legend="10-50 MA used for in-out of market")
addTA(GSPC.Slow.Diff, col='red', type='h',legend="50-200 MA give trending sense")
addTA=c(addMACD(),addADX(n = 14, maType = "EMA"))
GSPC.Long_Trades <- ifelse(
GSPC.SlowS.Diff > 0 &
GSPC.FastS.Diff > 0 &
shift(v=as.numeric(GSPC.FastS.Diff), places=1,dir = 'right') < 0,
GSPC$GSPC.Close, NA)
GSPC.Short_Trades <- ifelse(
GSPC.SlowS.Diff < 0 &
GSPC.FastS.Diff < 0 &
shift(v=as.numeric(GSPC.FastS.Diff), places=1,dir = 'right') > 0,
GSPC$GSPC.Close, NA)
plot(GSPC$GSPC.Close)
points(GSPC.Long_Trades, col='blue', cex=1.5, pch=18)
points(GSPC.Short_Trades, col='red', cex=1.5, pch=18)
addTA(GSPC.Fast.Diff, col='blue', type='h',legend="10-50 MA used for in-out of market")
addTA(GSPC.Slow.Diff, col='red', type='h',legend="50-200 MA give trending sense")
#addTA=c(addMACD(),addADX(n = 14, maType = "EMA"))
GSPC.Long_Trades <- ifelse(
GSPC.SlowS.Diff > 0 &
GSPC.FastS.Diff > 0 &
shift(v=as.numeric(GSPC.FastS.Diff), places=1,dir = 'right') < 0,
GSPC$GSPC.Close, NA)
GSPC.Short_Trades <- ifelse(
GSPC.SlowS.Diff < 0 &
GSPC.FastS.Diff < 0 &
shift(v=as.numeric(GSPC.FastS.Diff), places=1,dir = 'right') > 0,
GSPC$GSPC.Close, NA)
plot(GSPC$GSPC.Close)
points(GSPC.Long_Trades, col='blue', cex=1.5, pch=18)
points(GSPC.Short_Trades, col='red', cex=1.5, pch=18)
#addTA(GSPC.Fast.Diff, col='blue', type='h',legend="10-50 MA used for in-out of market")
#addTA(GSPC.Slow.Diff, col='red', type='h',legend="50-200 MA give trending sense")
#addTA=c(addMACD(),addADX(n = 14, maType = "EMA"))
GSPC.Long_Trades <- ifelse(
GSPC.SlowS.Diff > 0 &
GSPC.FastS.Diff > 0 &
shift(v=as.numeric(GSPC.FastS.Diff), places=1,dir = 'right') < 0,
GSPC$GSPC.Close, NA)
GSPC.Short_Trades <- ifelse(
GSPC.SlowS.Diff < 0 &
GSPC.FastS.Diff < 0 &
shift(v=as.numeric(GSPC.FastS.Diff), places=1,dir = 'right') > 0,
GSPC$GSPC.Close, NA)
plot(GSPC$GSPC.Close)
points(GSPC.Long_Trades, col='blue', cex=1.5, pch=18)
points(GSPC.Short_Trades, col='red', cex=1.5, pch=18)
#addTA(GSPC.Fast.Diff, col='blue', type='h',legend="10-50 MA used for in-out of market")
#addTA(GSPC.Slow.Diff, col='red', type='h',legend="50-200 MA give trending sense")
addTA=c(addMACD(),addADX(n = 14, maType = "EMA"))
GSPC.Long_Trades <- ifelse(
GSPC.SlowS.Diff > 0 &
GSPC.FastS.Diff > 0 &
shift(v=as.numeric(GSPC.FastS.Diff), places=1,dir = 'right') < 0,
GSPC$GSPC.Close, NA)
GSPC.Short_Trades <- ifelse(
GSPC.SlowS.Diff < 0 &
GSPC.FastS.Diff < 0 &
shift(v=as.numeric(GSPC.FastS.Diff), places=1,dir = 'right') > 0,
GSPC$GSPC.Close, NA)
plot(GSPC$GSPC.Close)
points(GSPC.Long_Trades, col='blue', cex=1.5, pch=18)
points(GSPC.Short_Trades, col='red', cex=1.5, pch=18)
#addTA(GSPC.Fast.Diff, col='blue', type='h',legend="10-50 MA used for in-out of market")
#addTA(GSPC.Slow.Diff, col='red', type='h',legend="50-200 MA give trending sense")
#addTA=c(addMACD(),addADX(n = 14, maType = "EMA"))
SPY.Long_Trades <- ifelse(
SPY.Slow.Diff > 0 &
SPY.Fast.Diff > 0 &
shift(v=as.numeric(SPY.FastS.Diff), places=1,dir = 'right') < 0,
SPY$SPY.Close, NA)
SPY.Long_Trades <- ifelse(
SPY.Slow.Diff > 0 &
SPY.Fast.Diff > 0 &
shift(v=as.numeric(SPY.Fast.Diff), places=1,dir = 'right') < 0,
SPY$SPY.Close, NA)
SPY.Short_Trades <- ifelse(
SPY.Slow.Diff < 0 &
SPY.Fast.Diff < 0 &
shift(v=as.numeric(SPY.Fast.Diff), places=1,dir = 'right') > 0,
SPY$SPY.Close, NA)
plot(SPY$SPY.Close)
points(SPY.Long_Trades, col='blue', cex=1.5, pch=18)
points(SPY.Short_Trades, col='red', cex=1.5, pch=18)
setwd("~/Machine_Learning/Machine Learning A-Z New/Part 3 - Classification/Section 18 - Naive Bayes")
getwd()
dataset = read.csv('Social_Network_Ads.csv')
dataset = dataset[3:5]
summary(dataset)
head(dataset)
dataset$Purchased = factor(dataset$Purchased, levels = c(0, 1))
summary(dataset)
# install.packages('caTools')
library(caTools)
set.seed(123)
split = sample.split(dataset$Purchased, SplitRatio = 0.75)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
training_set[-3] = scale(training_set[-3])
test_set[-3] = scale(test_set[-3])
head(training_set)
?e1071
library("e1071", lib.loc="/Library/Frameworks/R.framework/Versions/3.5/Resources/library")
# install.packages('e1071')
library(e1071)
classifier = naiveBayes(x = training_set[-3],
y = training_set$Purchased)
View(classifier)
y_predR = predict(classifier, newdata = test_set[-3])
y_pred = predict(classifier, newdata = test_set[-3])
y_pred
getwd()
dataset = read.csv('Social_Network_Ads.csv')
dataset = dataset[3:5]
summary(dataset)
head(dataset)
dataset$Purchased = factor(dataset$Purchased, levels = c(0, 1))
summary(dataset)
# install.packages('caTools')
library(caTools)
set.seed(123)
split = sample.split(dataset$Purchased, SplitRatio = 0.75)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
training_set[-3] = scale(training_set[-3])
test_set[-3] = scale(test_set[-3])
head(training_set)
# install.packages('e1071')
library(e1071)
classifier = naiveBayes(x = training_set[-3],
y = training_set$Purchased)
y_pred = predict(classifier, newdata = test_set[-3])
cm = table(test_set[, 3], y_pred)
cm
y_pred
library(ElemStatLearn)
# declare set as the training set
set = training_set
# this section creates the background region red/green. It does that by the 'by' which you can think of as the steps in python, so each 0.01 is interpreted as 0 or 1 and is either green or red. The -1 and +1 give us the space around the edges so the dots are not jammed
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
# just giving a name to the X and Y
colnames(grid_set) = c('Age', 'EstimatedSalary')
# this is the MAGIC of the background coloring
# here we use the classifier to predict the result of each of each of the pixel bits noted above
y_grid = predict(classifier, newdata = grid_set)
# that's the end of the background
# now we plat the actual data
plot(set[, -3],
main = 'Naive Bayes (Training set)',
xlab = 'Age', ylab = 'Estimated Salary',
xlim = range(X1), ylim = range(X2)) # this bit creates the limits to the values plotted this is also a part of the MAGIC as it creates the line between green and red
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
# here we run through all the y_pred data and use ifelse to color the dots
# note the dots are the real data, the background is the pixel by pixel determination of y/n
# graph the dots on top of the background give you the image
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
library(ElemStatLearn)
set = test_set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
y_grid = predict(classifier, newdata = grid_set)
plot(set[, -3], main = 'SVM Radial Kernel (Test set)',
xlab = 'Age', ylab = 'Estimated Salary',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
setwd("~/Machine_Learning/Machine Learning A-Z New/Part 3 - Classification/Section 19 - Decision Tree Classification")
getwd()
library("rpart", lib.loc="/Library/Frameworks/R.framework/Versions/3.5/Resources/library")
dataset = read.csv('Social_Network_Ads.csv')
dataset = dataset[3:5]
getwd()
dataset = read.csv('Social_Network_Ads.csv')
dataset = dataset[3:5]
summary(dataset)
head(dataset)
dataset$Purchased = factor(dataset$Purchased, levels = c(0, 1))
summary(dataset)
# install.packages('caTools')
library(caTools)
set.seed(123)
split = sample.split(dataset$Purchased, SplitRatio = 0.75)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
training_set[-3] = scale(training_set[-3])
test_set[-3] = scale(test_set[-3])
head(training_set)
training_set[-3] = scale(training_set[-3])
test_set[-3] = scale(test_set[-3])
head(training_set)
# install.packages('rpart')
library(rpart)
classifier = rpart(formula = Purchased ~ .,
data = training_set)
y_pred = predict(classifier, newdata = test_set[-3])
View(y_pred)
y_pred
head(y_pred)
cm = table(test_set[, 3], y_pred)
library(ElemStatLearn)
# declare set as the training set
set = training_set
# this section creates the background region red/green. It does that by the 'by' which you can think of as the steps in python, so each 0.01 is interpreted as 0 or 1 and is either green or red. The -1 and +1 give us the space around the edges so the dots are not jammed. Another way to think of the 'by' as is as the resolution of the graphing of the background
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
# just giving a name to the X and Y
colnames(grid_set) = c('Age', 'EstimatedSalary')
# this is the MAGIC of the background coloring
# here we use the classifier to predict the result of each of each of the pixel bits noted above. NOTE we need class here because we have a y_grid is a matrix!
y_grid = predict(classifier, newdata = grid_set, type = 'class')
# that's the end of the background
# now we plat the actual data
plot(set[, -3],
main = 'Decision Tree (Training set)',
xlab = 'Age', ylab = 'Estimated Salary',
xlim = range(X1), ylim = range(X2)) # this bit creates the limits to the values plotted this is also a part of the MAGIC as it creates the line between green and red
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
# here we run through all the y_pred data and use ifelse to color the dots
# note the dots are the real data, the background is the pixel by pixel determination of y/n
# graph the dots on top of the background give you the image
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
library(ElemStatLearn)
set = test_set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
# NOTE we need class here because we have a y_grid is a matrix!
y_grid = predict(classifier, newdata = grid_set, type = 'class')
plot(set[, -3], main = 'Decision Tree (Test set)',
xlab = 'Age', ylab = 'Estimated Salary',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
cm = table(test_set[, 3], y_pred, class = 'type')
cm = table(test_set[, 3], y_pred)
y_pred = predict(classifier, newdata = test_set[-3], type = 'class')
head(y_pred)
y_pred = predict(classifier, newdata = test_set[-3])
y_pred = predict(classifier, newdata = test_set[-3], type = 'class')
y_pred
y_pred = predict(classifier, newdata = test_set[-3])
head(y_pred)
y_pred = predict(classifier, newdata = test_set[-3], type = 'class')
y_pred
cm = table(test_set[, 3], y_pred)
cm
library(ElemStatLearn)
# declare set as the training set
set = training_set
# this section creates the background region red/green. It does that by the 'by' which you can think of as the steps in python, so each 0.01 is interpreted as 0 or 1 and is either green or red. The -1 and +1 give us the space around the edges so the dots are not jammed. Another way to think of the 'by' as is as the resolution of the graphing of the background
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
# just giving a name to the X and Y
colnames(grid_set) = c('Age', 'EstimatedSalary')
# this is the MAGIC of the background coloring
# here we use the classifier to predict the result of each of each of the pixel bits noted above. NOTE we need class here because we have a y_grid is a matrix!
y_grid = predict(classifier, newdata = grid_set, type = 'class')
# that's the end of the background
# now we plat the actual data
plot(set[, -3],
main = 'Decision Tree (Training set)',
xlab = 'Age', ylab = 'Estimated Salary',
xlim = range(X1), ylim = range(X2)) # this bit creates the limits to the values plotted this is also a part of the MAGIC as it creates the line between green and red
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
# here we run through all the y_pred data and use ifelse to color the dots
# note the dots are the real data, the background is the pixel by pixel determination of y/n
# graph the dots on top of the background give you the image
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
library(ElemStatLearn)
set = test_set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
# NOTE we need class here because we have a y_grid is a matrix!
y_grid = predict(classifier, newdata = grid_set, type = 'class')
plot(set[, -3], main = 'Decision Tree (Test set)',
xlab = 'Age', ylab = 'Estimated Salary',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
library(ElemStatLearn)
set = test_set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
# NOTE we need class here because we have a y_grid is a matrix!
y_grid = predict(classifier, newdata = grid_set, type = 'class')
plot(set[, -3], main = 'Decision Tree (Test set)',
xlab = 'Age', ylab = 'Estimated Salary',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
plot(classifier)
text(classifier)
plot(classifier)
text(classifier)
plot(classifier)
text(classifier)
getwd()
dataset = read.csv('Social_Network_Ads.csv')
dataset = dataset[3:5]
summary(dataset)
head(dataset)
dataset$Purchased = factor(dataset$Purchased, levels = c(0, 1))
summary(dataset)
# install.packages('caTools')
library(caTools)
set.seed(123)
split = sample.split(dataset$Purchased, SplitRatio = 0.75)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
View(training_set)
View(training_set)
View(test_set)
# install.packages('rpart')
library(rpart)
classifier = rpart(formula = Purchased ~ .,
data = training_set)
y_pred = predict(classifier, newdata = test_set[-3])
head(y_pred)
plot(classifier)
text(classifier)
dataset = read.csv('Social_Network_Ads.csv')
dataset = dataset[3:5]
dataset$Purchased = factor(dataset$Purchased, levels = c(0, 1))
# install.packages('caTools')
library(caTools)
set.seed(123)
split = sample.split(dataset$Purchased, SplitRatio = 0.75)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
plot(classifier)
text(classifier)
getwd()
dataset = read.csv('Social_Network_Ads.csv')
dataset = dataset[3:5]
summary(dataset)
head(dataset)
dataset$Purchased = factor(dataset$Purchased, levels = c(0, 1))
summary(dataset)
# install.packages('caTools')
library(caTools)
set.seed(123)
split = sample.split(dataset$Purchased, SplitRatio = 0.75)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
training_set[-3] = scale(training_set[-3])
test_set[-3] = scale(test_set[-3])
head(training_set)
# install.packages('rpart')
library(rpart)
classifier = rpart(formula = Purchased ~ .,
data = training_set)
y_pred = predict(classifier, newdata = test_set[-3])
head(y_pred)
y_pred = predict(classifier, newdata = test_set[-3], type = 'class')
y_pred
cm = table(test_set[, 3], y_pred)
cm
library(ElemStatLearn)
# declare set as the training set
set = training_set
# this section creates the background region red/green. It does that by the 'by' which you can think of as the steps in python, so each 0.01 is interpreted as 0 or 1 and is either green or red. The -1 and +1 give us the space around the edges so the dots are not jammed. Another way to think of the 'by' as is as the resolution of the graphing of the background
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
# just giving a name to the X and Y
colnames(grid_set) = c('Age', 'EstimatedSalary')
# this is the MAGIC of the background coloring
# here we use the classifier to predict the result of each of each of the pixel bits noted above. NOTE we need class here because we have a y_grid is a matrix!
y_grid = predict(classifier, newdata = grid_set, type = 'class')
# that's the end of the background
# now we plat the actual data
plot(set[, -3],
main = 'Decision Tree (Training set)',
xlab = 'Age', ylab = 'Estimated Salary',
xlim = range(X1), ylim = range(X2)) # this bit creates the limits to the values plotted this is also a part of the MAGIC as it creates the line between green and red
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
# here we run through all the y_pred data and use ifelse to color the dots
# note the dots are the real data, the background is the pixel by pixel determination of y/n
# graph the dots on top of the background give you the image
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
library(ElemStatLearn)
set = test_set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
# NOTE we need class here because we have a y_grid is a matrix!
y_grid = predict(classifier, newdata = grid_set, type = 'class')
plot(set[, -3], main = 'Decision Tree (Test set)',
xlab = 'Age', ylab = 'Estimated Salary',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
dataset = read.csv('Social_Network_Ads.csv')
dataset = dataset[3:5]
dataset$Purchased = factor(dataset$Purchased, levels = c(0, 1))
# install.packages('caTools')
library(caTools)
set.seed(123)
split = sample.split(dataset$Purchased, SplitRatio = 0.75)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
# install.packages('rpart')
library(rpart)
classifier = rpart(formula = Purchased ~ .,
data = training_set)
plot(classifier)
text(classifier)
