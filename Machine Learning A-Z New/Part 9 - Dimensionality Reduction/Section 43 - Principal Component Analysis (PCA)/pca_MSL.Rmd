---
title: "Dimensionality reduction using Principal Component Analysis (PCA)"
output: html_document
---
Lecture 264 PCA Intuition  
https://www.udemy.com/machinelearning/learn/lecture/10628128   

Great presentation and tutorial  
https://plot.ly/ipython-notebooks/principal-component-analysis/  

Another explanation of topic  
http://setosa.io/ev/principal-component-analysis/  
home page of this stuff http://setosa.io/ev/  

PCA Wikipedia https://en.wikipedia.org/wiki/Principal_component_analysis  

# PCA in a nutshell
What we are doing is taking a large number of independent variables and extracting them down to a core group of new independent variables that best describe the relationship (most variance) of the data in the dataset. Because this extraction is done without knowledge of the dependent variable the PCA process is considered un-supervised dimensionality reduction technique.

Both PCA and LDA are applicable to data that can be linearly separable. 

check working directory getwd()
```{r, include=FALSE}
getwd()
```

# Importing the dataset
```{r, include=TRUE}
dataset = read.csv('Wine.csv')
```

```{r, echo=TRUE, fig.cap="Python view of dataset", out.width = '100%'}
knitr::include_graphics("Datasetinformation.png")
```

# Splitting the dataset into the Training set and Test set
```{r , include=TRUE}
# install.packages('caTools')
library(caTools)
set.seed(123)
split = sample.split(dataset$Customer_Segment, SplitRatio = 0.8)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
```

# Feature Scaling
We'll scale all the Independent variables, not the customer segment (Dependent variable). 
```{r , include=TRUE}
training_set[-14] = scale(training_set[-14])
test_set[-14] = scale(test_set[-14])
```

# Applying PCA
Thresh is a cutoff for the cumulative percent of variance to be retained by PCA. We won't use this but if we wanted a particular cut off of explanation from our extracted features we'd use this parameter. pcaComp is the specific number of PCA components to keep. If specified, this over-rides thresh, we'll go with 2. 
```{r , include=TRUE}
# install.packages('caret')
library(caret)
# install.packages('e1071')
library(e1071)
# we will remove dependent variable from the training_set as PCA is an unsupervised dimensionality reduction technique
pca = preProcess(x = training_set[-14], method = 'pca', pcaComp = 2)
# we'll use predict to apply our pca object to our training_set
training_set = predict(pca, training_set)
# we need to put the columns back in order we want PC1, PC2, Customer_segment
training_set = training_set[c(2, 3, 1)]
test_set = predict(pca, test_set)
# we need to put the columns back in order we want
test_set = test_set[c(2, 3, 1)]
```
# PC1 and PC2
Principle Component 1 and 2 are our new variables. 
```{r , include=TRUE}
head(training_set)
```
# Fitting SVM to the Training set
```{r , include=TRUE}
# install.packages('e1071')
library(e1071)
classifier = svm(formula = Customer_Segment ~ .,
                 data = training_set,
                 type = 'C-classification',
                 kernel = 'linear')
```

# Predicting the Test set results
```{r , include=TRUE}
y_pred = predict(classifier, newdata = test_set[-3])
y_pred
```

# Making the Confusion Matrix
Perefect results
```{r , include=TRUE}
cm = table(test_set[, 3], y_pred)
cm
```

# Accuracy
```{r , include=TRUE}
accuracy = (cm[1,1] + cm[2,2]) / (cm[1,1] + cm[2,2] + cm[1,2] + cm[2,1])
  return(accuracy)
```

# Predicting the Training set results
```{r , include=TRUE}
y_predTR = predict(classifier, newdata = training_set[-3])
y_predTR
```

# Making the Confusion Matrix - Training set
Not bad, couple of mistakes
```{r , include=TRUE}
cmTR = table(training_set[, 3], y_predTR)
cmTR
```

# Visualising the Training set results
```{r , include=TRUE, fig.width=10, fig.height=7}
library(ElemStatLearn)
set = training_set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('PC1', 'PC2')
y_grid = predict(classifier, newdata = grid_set)
plot(set[, -3],
     main = 'Principal Component Analysis (PCA) (Training set)',
     xlab = 'PC1', ylab = 'PC2',
     xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 2, 'deepskyblue', ifelse(y_grid == 1, 'springgreen3', 'tomato')))
points(set, pch = 21, bg = ifelse(set[, 3] == 2, 'blue3', ifelse(set[, 3] == 1, 'green4', 'red3')))
```

# Visualising the Test set results
```{r , include=TRUE, fig.width=10, fig.height=7}
library(ElemStatLearn)
set = test_set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('PC1', 'PC2')
y_grid = predict(classifier, newdata = grid_set)
plot(set[, -3], main = 'Principal Component Analysis (PCA) (Test set)',
     xlab = 'PC1', ylab = 'PC2',
     xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 2, 'deepskyblue', ifelse(y_grid == 1, 'springgreen3', 'tomato')))
points(set, pch = 21, bg = ifelse(set[, 3] == 2, 'blue3', ifelse(set[, 3] == 1, 'green4', 'red3')))
```