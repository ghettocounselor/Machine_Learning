addTA(GSPC.Fast.Diff, col='blue', type='h',legend="10-50 MA used for in-out of market")
addTA(GSPC.Slow.Diff, col='red', type='h',legend="50-200 MA give trending sense")
addTA=c(addMACD(),addVo(),addADX(n = 14, maType = "EMA"))
getSymbols(c('^GSPC'), src='yahoo')
chartSeries(GSPC, theme=chartTheme('white'),
type = c("auto", "matchsticks"),
subset = '2007-01::2015',
show.grid = TRUE,
major.ticks='auto', minor.ticks=TRUE,
multi.col = FALSE,
TA="addEMA(50, col='black');addEMA(200, col='blue')")
GSPC.EMA.10 <- EMA(GSPC$GSPC.Close, n=10 )
GSPC.EMA.50 <- EMA(GSPC$GSPC.Close, n=50 )
GSPC.EMA.200 <- EMA(GSPC$GSPC.Close, n=200 )
GSPC.Fast.Diff <- GSPC.EMA.10 - GSPC.EMA.50
GSPC.Slow.Diff <- GSPC.EMA.50 - GSPC.EMA.200
addTA(GSPC.Fast.Diff, col='blue', type='h',legend="10-50 MA used for in-out of market")
addTA(GSPC.Slow.Diff, col='red', type='h',legend="50-200 MA give trending sense")
addTA=c(addMACD(),addVo(),addADX(n = 14, maType = "EMA"))
chartSeries(AMZN, theme=chartTheme('white'),
type = c("auto", "matchsticks"),
subset = '2016-01::',
show.grid = TRUE,
major.ticks='auto', minor.ticks=TRUE,
multi.col = FALSE,
TA="addEMA(50, col='black');addEMA(200, col='blue')")
AMZN.EMA.10 <- EMA(AMZN$AMZN.Close, n=10 )
AMZN.EMA.50 <- EMA(AMZN$AMZN.Close, n=50 )
AMZN.EMA.200 <- EMA(AMZN$AMZN.Close, n=200 )
AMZN.Fast.Diff <- AMZN.EMA.10 - AMZN.EMA.50
AMZN.Slow.Diff <- AMZN.EMA.50 - AMZN.EMA.200
addTA(AMZN.Fast.Diff, col='blue', type='h',legend="10-50 MA used for in-out of market")
addTA(AMZN.Slow.Diff, col='red', type='h',legend="50-200 MA give trending sense")
addTA=c(addMACD(),addVo(),addADX(n = 14, maType = "EMA"))
chartSeries(AMZN, theme=chartTheme('white'),
type = c("auto", "matchsticks"),
subset = '2016-01::',
show.grid = TRUE,
major.ticks='auto', minor.ticks=TRUE,
multi.col = FALSE,
TA="addEMA(50, col='black');addEMA(200, col='blue')")
plot(AMZN)
chartSeries(AMZN, theme=chartTheme('white'),
type = c("auto", "matchsticks"),
subset = '2016-01::',
show.grid = TRUE,
major.ticks='auto', minor.ticks=TRUE,
multi.col = FALSE,
TA="addEMA(50, col='black');addEMA(200, col='blue')")
plot(AMZN$AMZN.Close)
plot(GSPC$GSPC.Close)
GSPC.Long_Trades <- ifelse(
GSPC.SlowS.Diff > 0 &
GSPC.FastS.Diff > 0 &
shift(v=as.numeric(GSPC.FastS.Diff), places=1,dir = 'right') < 0,
GSPC$GSPC.Close, NA)
GSPC.Short_Trades <- ifelse(
GSPC.SlowS.Diff < 0 &
GSPC.FastS.Diff < 0 &
shift(v=as.numeric(GSPC.FastS.Diff), places=1,dir = 'right') > 0,
GSPC$GSPC.Close, NA)
plot(GSPC$GSPC.Close)
points(GSPC.Long_Trades, col='blue', cex=1.5, pch=18)
points(GSPC.Short_Trades, col='red', cex=1.5, pch=18)
GSPC.Long_Trades <- ifelse(
GSPC.SlowS.Diff > 0 &
GSPC.FastS.Diff > 0 &
shift(v=as.numeric(GSPC.FastS.Diff), places=1,dir = 'right') < 0,
GSPC$GSPC.Close, NA)
GSPC.Short_Trades <- ifelse(
GSPC.SlowS.Diff < 0 &
GSPC.FastS.Diff < 0 &
shift(v=as.numeric(GSPC.FastS.Diff), places=1,dir = 'right') > 0,
GSPC$GSPC.Close, NA)
plot(GSPC$GSPC.Close)
points(GSPC.Long_Trades, col='blue', cex=1.5, pch=18)
points(GSPC.Short_Trades, col='red', cex=1.5, pch=18)
addTA(GSPC.Fast.Diff, col='blue', type='h',legend="10-50 MA used for in-out of market")
addTA(GSPC.Slow.Diff, col='red', type='h',legend="50-200 MA give trending sense")
addTA=c(addMACD(),addVo(),addADX(n = 14, maType = "EMA"))
GSPC.Long_Trades <- ifelse(
GSPC.SlowS.Diff > 0 &
GSPC.FastS.Diff > 0 &
shift(v=as.numeric(GSPC.FastS.Diff), places=1,dir = 'right') < 0,
GSPC$GSPC.Close, NA)
GSPC.Short_Trades <- ifelse(
GSPC.SlowS.Diff < 0 &
GSPC.FastS.Diff < 0 &
shift(v=as.numeric(GSPC.FastS.Diff), places=1,dir = 'right') > 0,
GSPC$GSPC.Close, NA)
plot(GSPC$GSPC.Close)
points(GSPC.Long_Trades, col='blue', cex=1.5, pch=18)
points(GSPC.Short_Trades, col='red', cex=1.5, pch=18)
addTA(GSPC.Fast.Diff, col='blue', type='h',legend="10-50 MA used for in-out of market")
addTA(GSPC.Slow.Diff, col='red', type='h',legend="50-200 MA give trending sense")
addTA=c(addMACD(),addVo(),addADX(n = 14, maType = "EMA"))
GSPC.Long_Trades <- ifelse(
GSPC.SlowS.Diff > 0 &
GSPC.FastS.Diff > 0 &
shift(v=as.numeric(GSPC.FastS.Diff), places=1,dir = 'right') < 0,
GSPC$GSPC.Close, NA)
GSPC.Short_Trades <- ifelse(
GSPC.SlowS.Diff < 0 &
GSPC.FastS.Diff < 0 &
shift(v=as.numeric(GSPC.FastS.Diff), places=1,dir = 'right') > 0,
GSPC$GSPC.Close, NA)
plot(GSPC$GSPC.Close)
points(GSPC.Long_Trades, col='blue', cex=1.5, pch=18)
points(GSPC.Short_Trades, col='red', cex=1.5, pch=18)
addTA(GSPC.Fast.Diff, col='blue', type='h',legend="10-50 MA used for in-out of market")
addTA(GSPC.Slow.Diff, col='red', type='h',legend="50-200 MA give trending sense")
addTA=c(addMACD(),addADX(n = 14, maType = "EMA"))
GSPC.Long_Trades <- ifelse(
GSPC.SlowS.Diff > 0 &
GSPC.FastS.Diff > 0 &
shift(v=as.numeric(GSPC.FastS.Diff), places=1,dir = 'right') < 0,
GSPC$GSPC.Close, NA)
GSPC.Short_Trades <- ifelse(
GSPC.SlowS.Diff < 0 &
GSPC.FastS.Diff < 0 &
shift(v=as.numeric(GSPC.FastS.Diff), places=1,dir = 'right') > 0,
GSPC$GSPC.Close, NA)
plot(GSPC$GSPC.Close)
points(GSPC.Long_Trades, col='blue', cex=1.5, pch=18)
points(GSPC.Short_Trades, col='red', cex=1.5, pch=18)
addTA(GSPC.Fast.Diff, col='blue', type='h',legend="10-50 MA used for in-out of market")
addTA(GSPC.Slow.Diff, col='red', type='h',legend="50-200 MA give trending sense")
#addTA=c(addMACD(),addADX(n = 14, maType = "EMA"))
GSPC.Long_Trades <- ifelse(
GSPC.SlowS.Diff > 0 &
GSPC.FastS.Diff > 0 &
shift(v=as.numeric(GSPC.FastS.Diff), places=1,dir = 'right') < 0,
GSPC$GSPC.Close, NA)
GSPC.Short_Trades <- ifelse(
GSPC.SlowS.Diff < 0 &
GSPC.FastS.Diff < 0 &
shift(v=as.numeric(GSPC.FastS.Diff), places=1,dir = 'right') > 0,
GSPC$GSPC.Close, NA)
plot(GSPC$GSPC.Close)
points(GSPC.Long_Trades, col='blue', cex=1.5, pch=18)
points(GSPC.Short_Trades, col='red', cex=1.5, pch=18)
#addTA(GSPC.Fast.Diff, col='blue', type='h',legend="10-50 MA used for in-out of market")
#addTA(GSPC.Slow.Diff, col='red', type='h',legend="50-200 MA give trending sense")
#addTA=c(addMACD(),addADX(n = 14, maType = "EMA"))
GSPC.Long_Trades <- ifelse(
GSPC.SlowS.Diff > 0 &
GSPC.FastS.Diff > 0 &
shift(v=as.numeric(GSPC.FastS.Diff), places=1,dir = 'right') < 0,
GSPC$GSPC.Close, NA)
GSPC.Short_Trades <- ifelse(
GSPC.SlowS.Diff < 0 &
GSPC.FastS.Diff < 0 &
shift(v=as.numeric(GSPC.FastS.Diff), places=1,dir = 'right') > 0,
GSPC$GSPC.Close, NA)
plot(GSPC$GSPC.Close)
points(GSPC.Long_Trades, col='blue', cex=1.5, pch=18)
points(GSPC.Short_Trades, col='red', cex=1.5, pch=18)
#addTA(GSPC.Fast.Diff, col='blue', type='h',legend="10-50 MA used for in-out of market")
#addTA(GSPC.Slow.Diff, col='red', type='h',legend="50-200 MA give trending sense")
addTA=c(addMACD(),addADX(n = 14, maType = "EMA"))
GSPC.Long_Trades <- ifelse(
GSPC.SlowS.Diff > 0 &
GSPC.FastS.Diff > 0 &
shift(v=as.numeric(GSPC.FastS.Diff), places=1,dir = 'right') < 0,
GSPC$GSPC.Close, NA)
GSPC.Short_Trades <- ifelse(
GSPC.SlowS.Diff < 0 &
GSPC.FastS.Diff < 0 &
shift(v=as.numeric(GSPC.FastS.Diff), places=1,dir = 'right') > 0,
GSPC$GSPC.Close, NA)
plot(GSPC$GSPC.Close)
points(GSPC.Long_Trades, col='blue', cex=1.5, pch=18)
points(GSPC.Short_Trades, col='red', cex=1.5, pch=18)
#addTA(GSPC.Fast.Diff, col='blue', type='h',legend="10-50 MA used for in-out of market")
#addTA(GSPC.Slow.Diff, col='red', type='h',legend="50-200 MA give trending sense")
#addTA=c(addMACD(),addADX(n = 14, maType = "EMA"))
SPY.Long_Trades <- ifelse(
SPY.Slow.Diff > 0 &
SPY.Fast.Diff > 0 &
shift(v=as.numeric(SPY.FastS.Diff), places=1,dir = 'right') < 0,
SPY$SPY.Close, NA)
SPY.Long_Trades <- ifelse(
SPY.Slow.Diff > 0 &
SPY.Fast.Diff > 0 &
shift(v=as.numeric(SPY.Fast.Diff), places=1,dir = 'right') < 0,
SPY$SPY.Close, NA)
SPY.Short_Trades <- ifelse(
SPY.Slow.Diff < 0 &
SPY.Fast.Diff < 0 &
shift(v=as.numeric(SPY.Fast.Diff), places=1,dir = 'right') > 0,
SPY$SPY.Close, NA)
plot(SPY$SPY.Close)
points(SPY.Long_Trades, col='blue', cex=1.5, pch=18)
points(SPY.Short_Trades, col='red', cex=1.5, pch=18)
getwd()
getwd()
dataset_original = read.delim('Restaurant_Reviews.tsv', quote = '', stringsAsFactors = FALSE)
View(dataset_original)
install.packages('tm')
install.packages('SnowballC')
library(tm)
library(SnowballC)
corpus = VCorpus(VectorSource(dataset_original$Review))
View(corpus)
corpus = tm_map(corpus, content_transformer(tolower))
taile(corpus)
tail(corpus)
as.character(corpus[[1]])
corpus = tm_map(corpus, removeNumbers) # remove numbers
corpus = tm_map(corpus, removePunctuation)
as.character(corpus[[1]])
corpus = tm_map(corpus, removeWords, stopwords())
as.character(corpus[[1]])
corpus = tm_map(corpus, stemDocument)
as.character(corpus[[1]])
corpus = tm_map(corpus, stripWhitespace)
as.character(corpus[[1]])
as.character(corpus[[841]])
corpus = tm_map(corpus, content_transformer(tolower)) # make all words lower case
corpus = tm_map(corpus, removeNumbers) # remove numbers
corpus = tm_map(corpus, removePunctuation) # take out puncutation
corpus = tm_map(corpus, removeWords, stopwords()) # use stopwards library to clean off words
corpus = tm_map(corpus, stemDocument) # stem will transalte loved, loving, loves, etc... to love
corpus = tm_map(corpus, stripWhitespace) # some of the above steps will have added extra spaces into our data
dtm = DocumentTermMatrix(corpus)
dtm
dtm
dtm = removeSparseTerms(dtm, 0.999) # here we'll remove some of the less frequent words dtm
dtm
nrow(dtm)
ncol(dtm)
dataset = as.data.frame(as.matrix(dtm))
head(dataset)
head(dataset, 2)
# 1st bit adds new column Liked and 2nd bit gets the data from the original dataset data
dataset$Liked = dataset_original$Liked
dataset$Liked = factor(dataset$Liked, levels = c(0, 1))
# install.packages('caTools')
library(caTools)
set.seed(123)
split = sample.split(dataset$Liked, SplitRatio = 0.8)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
# install.packages('randomForest')
library(randomForest)
classifier = randomForest(x = training_set[-688], # in x we need to remove the dependent variable which is the last column liked, in our case 688, in lecture it was 692
y = training_set$Liked,
ntree = 10) # number of trees
y_pred = predict(classifier, newdata = test_set[-688])
cm = table(test_set[, 688], y_pred)
cm
knitr::include_graphics("Confusion_Matrix_Explained.png")
(76+71)/200
(76+71)/200
76/(76+24)
76/(76+29)
(2*0.76*0.7238095)/(0.76+0.7238095)
import numpy as np
getwd()
dataset = read.csv('Ads_CTR_Optimisation.csv')
knitr::include_graphics("UCB_compare_ThompsonSampling.png")
knitr::include_graphics("Dataset_interpretation.png")
N = 10000
d = 10
ads_selected = integer(0)
total_reward = 0
for (n in 1:N) {
ad = sample(1:10, 1)
ads_selected = append(ads_selected, ad)
reward = dataset[n, ad]
total_reward = total_reward + reward
}
total_reward
hist(ads_selected,
col = 'blue',
main = 'Histogram of ads selections - RANDOM',
xlab = 'Ads',
ylab = 'Number of times each ad was selected')
knitr::include_graphics("Thompson_Sampling_Slide.png")
N = 10000
d = 10
ads_selected = integer(0)
# UCB and Thompson Sampling algorithm are very similar but use different variables
# those variables are here
numbers_of_rewards_1 = integer(d) # the d defined above sets the initial as 10
numbers_of_rewards_0 = integer(d)
# These two variables will be put in place in the for loops
total_reward = 0
for (n in 1:N) {
ad = 0
max_random = 0
for (i in 1:d) {
random_beta = rbeta(n = 1,
shape1 = numbers_of_rewards_1[i] + 1,
shape2 = numbers_of_rewards_0[i] + 1)
if (random_beta > max_random) {
max_random = random_beta
ad = i
}
}
ads_selected = append(ads_selected, ad)
reward = dataset[n, ad]
if (reward == 1) {
numbers_of_rewards_1[ad] = numbers_of_rewards_1[ad] + 1
} else {
numbers_of_rewards_0[ad] = numbers_of_rewards_0[ad] + 1
}
total_reward = total_reward + reward
}
total_reward
knitr::include_graphics("bayesian_inference.png")
tail(ads_selected, n = 1000)
hist(ads_selected,
col = 'blue',
main = 'Histogram of ads selections (Thompson Sampling R)',
xlab = 'Ads',
ylab = 'Number of times each ad was selected')
hist(ads_selected,
col = 'blue',
main = 'Histogram of ads selections (Thompson Sampling R)',
xlab = 'Ads',
ylab = 'Number of times each ad was selected')
plt.txt(total_reward)
hist(ads_selected,
col = 'blue',
main = 'Histogram of ads selections (Thompson Sampling R)',
xlab = 'Ads',
ylab = 'Number of times each ad was selected',
plt.txt(total_reward))
hist(ads_selected,
col = 'blue',
main = 'Histogram of ads selections (Thompson Sampling R)',
xlab = 'Ads',
ylab = 'Number of times each ad was selected')
getwd()
setwd("~/Machine_Learning/Machine Learning A-Z New/Part 9 - Dimensionality Reduction/Section 44 - Linear Discriminant Analysis (LDA)")
dataset = read.csv('Wine.csv')
knitr::include_graphics("Datasetinformation.png")
# install.packages('caTools')
library(caTools)
set.seed(123)
split = sample.split(dataset$Customer_Segment, SplitRatio = 0.8)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
knitr::include_graphics("LDAvPCA.png")
training_set[-14] = scale(training_set[-14])
test_set[-14] = scale(test_set[-14])
library(MASS)
lda = lda(formula = Customer_Segment ~ ., data = training_set)
training_set = as.data.frame(predict(lda, training_set))
View(training_set)
head(training_set)
training_set = training_set[c(5, 6, 1)] # we need to get the columns in the right order
test_set = as.data.frame(predict(lda, test_set)) # LDA needs a datafram, in PCA we got a dataframe in our preprocessing
test_set = test_set[c(5, 6, 1)]
head(training_set)
# install.packages('e1071')
library(e1071)
classifier = svm(formula = class ~ .,  # note the customer segment is now called class
data = training_set,
type = 'C-classification',
kernel = 'linear')
y_pred = predict(classifier, newdata = test_set[-3])
cm = table(test_set[, 3], y_pred)
cm = table(test_set[, 3], y_pred)
cm
library(ElemStatLearn)
set = training_set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('x.LD1', 'x.LD2')  # note here we need the real names of the extracted features
y_grid = predict(classifier, newdata = grid_set)
plot(set[, -3],
main = 'Linear Discriminant Analysis (LDA) (Training set)',
xlab = 'LD1', ylab = 'LD2',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 2, 'deepskyblue', ifelse(y_grid == 1, 'springgreen3', 'tomato')))
points(set, pch = 21, bg = ifelse(set[, 3] == 2, 'blue3', ifelse(set[, 3] == 1, 'green4', 'red3')))
library(ElemStatLearn)
set = test_set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('x.LD1', 'x.LD2') # note here we need the real names of the extracted features
y_grid = predict(classifier, newdata = grid_set)
plot(set[, -3], main = 'Linear Discriminant Analysis (LDA) (Test set)',
xlab = 'LD1', ylab = 'LD2',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 2, 'deepskyblue', ifelse(y_grid == 1, 'springgreen3', 'tomato')))
points(set, pch = 21, bg = ifelse(set[, 3] == 2, 'blue3', ifelse(set[, 3] == 1, 'green4', 'red3')))
setwd("~/Machine_Learning/Machine Learning A-Z New/Part 9 - Dimensionality Reduction/Section 45 - Kernel PCA")
knitr::include_graphics("LogisticRegression_dataExplained.png")
getwd()
knitr::include_graphics("LogisticRegression_dataExplained.png")
getwd()
dataset = read.csv('Social_Network_Ads.csv')
dataset = dataset[, 3:5]
# install.packages('caTools')
library(caTools)
set.seed(123)
split = sample.split(dataset$Purchased, SplitRatio = 0.75)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
training_set[, 1:2] = scale(training_set[, 1:2])
test_set[, 1:2] = scale(test_set[, 1:2])
install.packages('kernlab')
library(kernlab)
kpca = kpca(~., data = training_set[-3], kernel = 'rbfdot', features = 2)
training_set_pca = as.data.frame(predict(kpca, training_set))
View(training_set_pca)
head(training_set_pca)
training_set_pca$Purchased = training_set$Purchased
head(training_set_pca)
test_set_pca = as.data.frame(predict(kpca, test_set))
test_set_pca$Purchased = test_set$Purchased
classifier = glm(formula = Purchased ~ .,
family = binomial,
data = training_set_pca)
prob_pred = predict(classifier, type = 'response', newdata = test_set_pca[-3])
y_pred = ifelse(prob_pred > 0.5, 1, 0)
cm = table(test_set_pca[, 3], y_pred)
cm
knitr::include_graphics("Confusion_Matrix_Explained.png")
knitr::include_graphics("TheKernelTrick.png")
knitr::include_graphics("KernelTrickMathVis.png")
setwd("~/Machine_Learning/Machine Learning A-Z New/Part 10 - Model Selection & Boosting/Section 48 - Model Selection")
getwd()
knitr::include_graphics("k-fold_crossValidation.png.png")
knitr::include_graphics("k-fold_crossValidation.png.png")
knitr::include_graphics("k-fold_crossValidation.png.png")
knitr::include_graphics("k-fold_crossValidation.png.png")
knitr::include_graphics("k-fold_crossValidation.png")
knitr::include_graphics("Bias_varianceTradeoff.png")
getwd()
dataset = read.csv('Social_Network_Ads.csv')
dataset = dataset[3:5]
dataset$Purchased = factor(dataset$Purchased, levels = c(0, 1))
# install.packages('caTools')
library(caTools)
set.seed(123)
split = sample.split(dataset$Purchased, SplitRatio = 0.75)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
training_set[-3] = scale(training_set[-3])
test_set[-3] = scale(test_set[-3])
# install.packages('e1071')
library(e1071)
classifier = svm(formula = Purchased ~ .,
data = training_set,
type = 'C-classification',
kernel = 'radial')
y_pred = predict(classifier, newdata = test_set[-3])
cm = table(test_set[, 3], y_pred)
cm = table(test_set[, 3], y_pred)
cm
# install.packages('caret')
library(caret)
# in creating the folds we specify the target feature and # of folds
folds = createFolds(training_set$Purchased, k = 10)
# in cv we are going to applying a created function to our 'folds'
cv = lapply(folds, function(x) { # start of function
# in the next two lines we will separate the Training set into it's 10 pieces
training_fold = training_set[-x, ] # training fold =  training set minus (-) it's sub test fold
test_fold = training_set[x, ] # here we describe the test fold individually
# now apply (train) the classifer on the training_fold
classifier = svm(formula = Purchased ~ .,
data = training_fold,
type = 'C-classification',
kernel = 'radial')
# next step in the loop, we calculate the predictions and cm and we equate the accuracy
# note we are training on training_fold and testing its accuracy on the test_fold
y_pred = predict(classifier, newdata = test_fold[-3])
cm = table(test_fold[, 3], y_pred)
accuracy = (cm[1,1] + cm[2,2]) / (cm[1,1] + cm[2,2] + cm[1,2] + cm[2,1])
return(accuracy)
})
accuracy = mean(as.numeric(cv))
View(cv)
knitr::include_graphics("CV.png")
accuracy = mean(as.numeric(cv))
accuracy = mean(as.numeric(cv))
accuracy
library(ElemStatLearn)
set = training_set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
y_grid = predict(classifier, newdata = grid_set)
plot(set[, -3],
main = 'Kernel SVM (Training set)',
xlab = 'Age', ylab = 'Estimated Salary',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
library(ElemStatLearn)
set = test_set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
y_grid = predict(classifier, newdata = grid_set)
plot(set[, -3], main = 'Kernel SVM (Test set)',
xlab = 'Age', ylab = 'Estimated Salary',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
